{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1779b0eb",
   "metadata": {},
   "source": [
    "## **Notebook on training the tokenizer for smol-llama**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755f320c",
   "metadata": {},
   "source": [
    "### **Install the deps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2e13a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install tqdm\n",
    "%uv pip install numpy\n",
    "%uv pip install torch\n",
    "%uv pip install wandb\n",
    "%uv pip install duckdb\n",
    "%uv pip install psutil\n",
    "%uv pip install pyarrow\n",
    "%uv pip install datasets\n",
    "%uv pip install tokenizers\n",
    "%uv pip install transformers\n",
    "%uv pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fb0077",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284af8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import duckdb\n",
    "import psutil\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660e9f9b",
   "metadata": {},
   "source": [
    "### **[SLOW STEP] Loading the dataset**\n",
    "\n",
    "- Here, we are simply downloading the dataset from hf (This step is slow, bottlenecked by internet speed)\n",
    "- We are not processing the file once it's downloaded\n",
    "  - Important, because this step blows up RAM usage\n",
    "- We'll use `duckdb` to read the `.parquet` file which is significantly faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c6d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_file = hf_hub_download(repo_id=\"ifkash/fineweb-6b\", filename=\"fineweb-6b.parquet\", repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9932b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect()\n",
    "\n",
    "with open(\"corpus_sample.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "  result = con.execute(f\"\"\"\n",
    "    SELECT text\n",
    "    FROM read_parquet('{pq_file}')\n",
    "    LIMIT 200000\n",
    "  \"\"\").fetchall()\n",
    "\n",
    "  for row in result:\n",
    "    file.write(row[0] + \"\\n\\n\")\n",
    "\n",
    "print(\"Sample saved as corpus_sample.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aa7e7f",
   "metadata": {},
   "source": [
    "### **Train the Byte-Level BPE**\n",
    "\n",
    "- In the above cells, we took 200K rows from the dataset\n",
    "- We'll train a tokenizer on that 200K subset\n",
    "\n",
    "**Why train the tokenizer on 200K rows, instead of the entire dataset?**\n",
    "- 200K rows is usually fine for byte-level BPE tokenizer\n",
    "- Using the full dataset gives a diminishing returns and it's often not worth the cost\n",
    "\n",
    "**Why does this work?**\n",
    "- Byte-level BPE starts from bytes $(0-255)$ and not words, this means:\n",
    "  - No out-of-vocab problem\n",
    "  - Any text, no matter how weird, can always be presented\n",
    "\n",
    "**What BPE learns?**\n",
    "- Which bytes sequences should be merged\n",
    "  - common subwords\n",
    "  - morphemes\n",
    "  - punctuation patterns\n",
    "\n",
    "**What happens if the tokenizer encounters \"new tokens\" later?**\n",
    "> Nothing breaks. Ever.\n",
    "\n",
    "Example: `helloüëÅÔ∏è‚Äçüó®Ô∏èworld_42_newThing`\n",
    "- The tokenizer will fallback to smaller merges\n",
    "- Ultimately fall back to raw bytes\n",
    "\n",
    "So, instead of:\n",
    "```\n",
    "[\"hello\", \"world\"]\n",
    "```\n",
    "We might get:\n",
    "```\n",
    "[\"h\", \"e\", \"l\", \"l\", \"o\", \"üëÅ\", \"Ô∏è\", \"‚Äç\", \"üó®\", \"Ô∏è\", ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9271c184",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = \"smol-llama-tokenizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b66f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "tokenizer.train(\n",
    "  files=[\"corpus_sample.txt\"],\n",
    "  vocab_size=49152,\n",
    "  min_frequency=2,\n",
    "  special_tokens=[\n",
    "    \"<|endoftext|>\",  # 0\n",
    "    \"<pad>\",  # 1\n",
    "    \"<s>\",  # 2\n",
    "  ],\n",
    ")\n",
    "\n",
    "tokenizer.save_model(\".\", tokenizer_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6697a7e",
   "metadata": {},
   "source": [
    "### **Wrap it for huggingface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2f151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_object = ByteLevelBPETokenizer(f\"{tokenizer_name}-vocab.json\", f\"{tokenizer_name}-merges.txt\")\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "  tokenizer_object=tokenizer_object,\n",
    "  bos_token=\"<s>\",\n",
    "  eos_token=\"<|endoftext|>\",\n",
    "  pad_token=\"<pad>\",\n",
    "  unk_token=\"<|endoftext|>\",  # ByteLevel doesn't really have unk, but good to map it\n",
    ")\n",
    "\n",
    "encoded = wrapped_tokenizer.encode(\"Hello world! This is my LLM.\")\n",
    "print(f\"Tokens: {encoded}\")\n",
    "print(f\"Decoded: {wrapped_tokenizer.decode(encoded)}\")\n",
    "\n",
    "wrapped_tokenizer.save_pretrained(f\"./{tokenizer_name}-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2964f",
   "metadata": {},
   "source": [
    "### **[SLOW STEP] Prepare data**\n",
    "\n",
    "- We read the `.parquet` file we downloaded earlier\n",
    "- We tokenizer every single document using the tokenizer we trainer earlier\n",
    "- Write the token IDs to 2 binary `.bin` files (`train.bin` and `val.bin`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab19af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARQUET_FILE = pq_file\n",
    "TOKENIZER_PATH = f\"./{tokenizer_name}-final\"\n",
    "OUTPUT_DIR = \"data_bin\"\n",
    "BATCH_SIZE = 10000\n",
    "TEST_SIZE = 0.005\n",
    "NUM_PROC = os.cpu_count()\n",
    "NUM_WORKERS = min(os.cpu_count(), 96)  # I ran it on AMD EPYC hence the 96\n",
    "CHUNK_SIZE = 50000\n",
    "METADATA_FILE = \"metadata.json\"\n",
    "VAL_SPLIT_PROB = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5eebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_batch(args):\n",
    "  texts, tokenizer_path, eos_id = args\n",
    "  tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)\n",
    "  result = tokenizer(texts, add_special_tokens=False)\n",
    "  tokens = []\n",
    "  for doc_ids in result.input_ids:\n",
    "    tokens.extend(doc_ids)\n",
    "    tokens.append(eos_id)\n",
    "  return np.array(tokens, dtype=np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81081f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "  os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "  print(f\"Loading tokenizer from {TOKENIZER_PATH}...\")\n",
    "  tokenizer = PreTrainedTokenizerFast.from_pretrained(TOKENIZER_PATH)\n",
    "  if tokenizer.eos_token is None:\n",
    "    tokenizer.eos_token = \"<|endoftext|>\"\n",
    "  eos_id = tokenizer.eos_token_id\n",
    "  print(f\"Using {NUM_WORKERS} CPU cores\")\n",
    "\n",
    "  print(f\"Opening parquet file: {PARQUET_FILE}...\")\n",
    "  parquet_file = pq.ParquetFile(PARQUET_FILE)\n",
    "  total_rows = parquet_file.metadata.num_rows\n",
    "  print(f\"Total documents: {total_rows:,}\")\n",
    "\n",
    "  print(\"Tokenizing and writing to temporary file...\")\n",
    "  temp_file = os.path.join(OUTPUT_DIR, \"temp_all_tokens.bin\")\n",
    "  total_tokens = 0\n",
    "\n",
    "  with (\n",
    "    open(temp_file, \"wb\") as f,\n",
    "    ProcessPoolExecutor(max_workers=NUM_WORKERS) as executor,\n",
    "  ):\n",
    "    processed_docs = 0\n",
    "\n",
    "    for batch in tqdm(\n",
    "      parquet_file.iter_batches(batch_size=CHUNK_SIZE),\n",
    "      total=(total_rows // CHUNK_SIZE) + 1,\n",
    "      desc=\"Processing\",\n",
    "    ):\n",
    "      texts = batch.column(\"text\").to_pylist()\n",
    "\n",
    "      worker_chunk_size = max(1, len(texts) // NUM_WORKERS)\n",
    "      text_chunks = [texts[i : i + worker_chunk_size] for i in range(0, len(texts), worker_chunk_size)]\n",
    "\n",
    "      args_list = [(chunk, TOKENIZER_PATH, eos_id) for chunk in text_chunks]\n",
    "\n",
    "      futures = [executor.submit(tokenize_batch, args) for args in args_list]\n",
    "\n",
    "      for future in as_completed(futures):\n",
    "        token_array = future.result()\n",
    "        token_array.tofile(f)\n",
    "        total_tokens += len(token_array)\n",
    "\n",
    "      processed_docs += len(texts)\n",
    "\n",
    "      if processed_docs % 100000 < CHUNK_SIZE:\n",
    "        print(f\"  {processed_docs:,} docs, {total_tokens / 1e9:.2f}B tokens\")\n",
    "\n",
    "  print(f\"\\nTotal tokens: {total_tokens / 1e9:.2f}B\")\n",
    "\n",
    "  print(\"Splitting into train and val...\")\n",
    "  val_size = int(total_tokens * TEST_SIZE)\n",
    "  train_size = total_tokens - val_size\n",
    "\n",
    "  print(f\"Train: {train_size / 1e9:.2f}B ({train_size / total_tokens * 100:.1f}%)\")\n",
    "  print(f\"Val: {val_size / 1e6:.2f}M ({val_size / total_tokens * 100:.1f}%)\")\n",
    "\n",
    "  print(\"Creating train and val files...\")\n",
    "  all_tokens = np.memmap(temp_file, dtype=np.uint16, mode=\"r\", shape=(total_tokens,))\n",
    "\n",
    "  train_file = os.path.join(OUTPUT_DIR, \"train.bin\")\n",
    "  val_file = os.path.join(OUTPUT_DIR, \"val.bin\")\n",
    "\n",
    "  train_tokens = all_tokens[:train_size]\n",
    "  with open(train_file, \"wb\") as f:\n",
    "    train_tokens.tofile(f)\n",
    "\n",
    "  val_tokens = all_tokens[train_size : train_size + val_size]\n",
    "  with open(val_file, \"wb\") as f:\n",
    "    val_tokens.tofile(f)\n",
    "\n",
    "  del all_tokens\n",
    "  os.remove(temp_file)\n",
    "\n",
    "  train_size_gb = os.path.getsize(train_file) / 1e9\n",
    "  val_size_mb = os.path.getsize(val_file) / 1e6\n",
    "\n",
    "  print(f\"{train_file} - {train_size_gb:.2f} GB\")\n",
    "  print(f\"{val_file} - {val_size_mb:.2f} MB\")\n",
    "\n",
    "\n",
    "process_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3328fe31",
   "metadata": {},
   "source": [
    "### **[OPTIONAL] Login to hf and push tokens to hf**\n",
    "\n",
    "- Go to https://huggingface.co/settings/tokens\n",
    "- Create a new token\n",
    "- Make sure it's \"write\"\n",
    "- Copy the token\n",
    "- Paste the token in the bottom cell, replacing `XXXXXXXXX`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a54e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv run hf auth login --token XXXXXXXXX\n",
    "%uv run hf upload ifkash/fineweb-tiny-processed ./data_bin . --repo-type dataset\n",
    "%uv run hf upload ifkash/fineweb-tiny-processed ./smol-llama-tokenizer-final . --repo-type dataset"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
